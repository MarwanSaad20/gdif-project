# ============================
# ğŸ§¼ Data Cleaning Experiments
# ============================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from pathlib import Path
import sys

# ======================
# ğŸ§© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ù‚Ù‚
# ======================
try:
    project_root = Path(__file__).resolve().parents[1]
except NameError:
    project_root = Path.cwd().parents[1]

sys.path.append(str(project_root / "data_intelligence_system"))

from data.processed.clean_preview import preview
from data.processed.validate_clean_data import validate  # âœ… Ù…Ø¶Ø§Ù Ø­Ø¯ÙŠØ«Ù‹Ø§


# ======================
# ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ======================

def load_data():
    data_path = project_root / "data_intelligence_system" / "data" / "processed" / "clean_data.csv"
    if not data_path.exists():
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {data_path}")
        return None, project_root
    df = pd.read_csv(data_path)
    print(f"âœ… Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df.shape}")
    preview(df)
    try:
        validate(df)  # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©
    except Exception as e:
        print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    return df, project_root


# ==========================
# ğŸ“‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
# ==========================

def handle_missing_values(df):
    for col in df.select_dtypes(include=np.number).columns:
        df[col] = df[col].fillna(df[col].mean())

    for col in df.select_dtypes(include='object').columns:
        if df[col].isnull().sum() > 0:
            df[col] = df[col].fillna(df[col].mode()[0])

    print(f"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {df.isnull().sum().sum()}")
    return df


# =====================
# ğŸ§± ØªÙˆØ­ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# =====================

def standardize_column_names(df):
    df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
    return df


# =====================
# ğŸ”  ØªØ±Ù…ÙŠØ² Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ÙˆØ¹ÙŠØ©
# =====================

def encode_categorical_columns(df):
    cat_cols = df.select_dtypes(include='object').columns.tolist()
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)
    return df


# ======================
# ğŸ“Š Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
# ======================

def scale_numeric_columns(df):
    num_cols = df.select_dtypes(include=np.number).columns
    scaler = MinMaxScaler()
    df[num_cols] = scaler.fit_transform(df[num_cols])
    return df, num_cols


# ======================
# âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©
# ======================

def check_quality(df, num_cols):
    print("ğŸ§ª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø·Ø§Ù‚ Ø£ÙˆÙ„ 5 Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ©:")
    for col in num_cols[:5]:
        print(f"{col}: min={df[col].min():.2f}, max={df[col].max():.2f}")


# ========================
# ğŸ“ˆ Ø±Ø³Ù… Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
# ========================

def plot_distributions(df, num_cols):
    df[num_cols[:5]].hist(figsize=(14, 8), bins=30)
    plt.suptitle("ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø£ÙˆÙ„ 5 Ø£Ø¹Ù…Ø¯Ø© Ø±Ù‚Ù…ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø¬ÙŠÙ…", fontsize=16, y=1.02)
    plt.tight_layout()
    plt.show()


# ========================
# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©
# ========================

def save_clean_data(df, project_root):
    output_path = project_root / "data_intelligence_system" / "data" / "processed" / "clean_data_transformed.csv"
    df.to_csv(output_path, index=False)
    print(f"[âœ“] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ: {output_path}")


# ========================
# ğŸš€ Ù†Ù‚Ø·Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª
# ========================

if __name__ == "__main__":
    df, root = load_data()
    if df is not None:
        df = handle_missing_values(df)
        df = standardize_column_names(df)
        df = encode_categorical_columns(df)
        df, num_cols = scale_numeric_columns(df)
        check_quality(df, num_cols)
        plot_distributions(df, num_cols)
        save_clean_data(df, root)
