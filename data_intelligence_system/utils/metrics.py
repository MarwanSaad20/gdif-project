import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.figure_factory as ff
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error,
    accuracy_score, f1_score, precision_score,
    recall_score, confusion_matrix, classification_report
)

from data_intelligence_system.utils.logger import get_logger

logger = get_logger(name="Metrics")


def regression_metrics(y_true: np.ndarray | list, y_pred: np.ndarray | list) -> dict:
    """
    Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø±: MAE, MSE, RMSE

    Args:
        y_true (array-like): Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ø±Ù‚Ù…ÙŠØ©).
        y_pred (array-like): Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (Ø±Ù‚Ù…ÙŠØ©).

    Returns:
        dict: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ MAE, MSE, RMSE Ù…Ø¹ ØªÙ‚Ø±ÙŠØ¨ 4 Ø£Ø±Ù‚Ø§Ù… Ø¹Ø´Ø±ÙŠØ©.

    Raises:
        ValueError: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª NoneØŒ ÙØ§Ø±ØºØ©ØŒ Ø£Ùˆ Ø°Ø§Øª Ø£Ø·ÙˆØ§Ù„ Ù…Ø®ØªÙ„ÙØ©.
    """
    if y_true is None or y_pred is None:
        logger.error("y_true Ø£Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† None.")
        raise ValueError("y_true Ùˆ y_pred ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙƒÙˆÙ†Ø§ None.")
    if len(y_true) == 0 or len(y_pred) == 0:
        logger.error("y_true Ø£Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ©.")
        raise ValueError("y_true Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ©.")
    if len(y_true) != len(y_pred):
        logger.error("Ø·ÙˆÙ„ y_true Ù„Ø§ ÙŠØ³Ø§ÙˆÙŠ Ø·ÙˆÙ„ y_pred.")
        raise ValueError("y_true Ùˆ y_pred ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ†Ø§ Ø¨Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„.")

    y_true_arr = np.array(y_true)
    y_pred_arr = np.array(y_pred)

    mae = mean_absolute_error(y_true_arr, y_pred_arr)
    mse = mean_squared_error(y_true_arr, y_pred_arr)
    rmse = np.sqrt(mse)

    return {
        "MAE": round(mae, 4),
        "MSE": round(mse, 4),
        "RMSE": round(rmse, 4)
    }


def classification_metrics(
    y_true: np.ndarray | list,
    y_pred: np.ndarray | list,
    average: str = 'weighted',
    zero_division: int = 0
) -> dict:
    """
    Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.

    Args:
        y_true (array-like): Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©.
        y_pred (array-like): Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.
        average (str): Ø·Ø±ÙŠÙ‚Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ù„Ù„Ù€ Precision, Recall, F1.
        zero_division (int): Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±.

    Returns:
        dict: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø¹ ØªÙ‚Ø±ÙŠØ¨ 4 Ø£Ø±Ù‚Ø§Ù… Ø¹Ø´Ø±ÙŠØ©ØŒ
              Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ ÙˆØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ (ÙƒÙ‚Ø§Ù…ÙˆØ³).
    
    Raises:
        ValueError: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª NoneØŒ ÙØ§Ø±ØºØ©ØŒ Ø£Ùˆ Ø°Ø§Øª Ø£Ø·ÙˆØ§Ù„ Ù…Ø®ØªÙ„ÙØ©.
    """
    if y_true is None or y_pred is None:
        logger.error("y_true Ø£Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† None.")
        raise ValueError("y_true Ùˆ y_pred ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙƒÙˆÙ†Ø§ None.")
    if len(y_true) == 0 or len(y_pred) == 0:
        logger.error("y_true Ø£Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ©.")
        raise ValueError("y_true Ùˆ y_pred Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ÙØ§Ø±ØºØ©.")
    if len(y_true) != len(y_pred):
        logger.error("Ø·ÙˆÙ„ y_true Ù„Ø§ ÙŠØ³Ø§ÙˆÙŠ Ø·ÙˆÙ„ y_pred.")
        raise ValueError("y_true Ùˆ y_pred ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ†Ø§ Ø¨Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„.")

    y_true_arr = np.array(y_true)
    y_pred_arr = np.array(y_pred)

    acc = accuracy_score(y_true_arr, y_pred_arr)
    f1 = f1_score(y_true_arr, y_pred_arr, average=average, zero_division=zero_division)
    precision = precision_score(y_true_arr, y_pred_arr, average=average, zero_division=zero_division)
    recall = recall_score(y_true_arr, y_pred_arr, average=average, zero_division=zero_division)
    cm = confusion_matrix(y_true_arr, y_pred_arr)
    report = classification_report(y_true_arr, y_pred_arr, output_dict=True, zero_division=zero_division)

    return {
        "Accuracy": round(acc, 4),
        "F1 Score": round(f1, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "Confusion Matrix": cm,
        "Classification Report": report
    }


def plot_confusion_matrix(cm: np.ndarray, labels: list | None = None,
                          title: str = "Confusion Matrix",
                          save_path: str | None = None) -> None:
    """
    Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Seaborn.

    Args:
        cm (np.ndarray): Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³.
        labels (list, optional): ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª.
        title (str, optional): Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø³Ù….
        save_path (str, optional): Ù…Ø³Ø§Ø± Ù„Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© (png/pdf).

    Raises:
        ValueError: Ø¥Ø°Ø§ ÙƒØ§Ù† cm Ù„ÙŠØ³Øª Ù…ØµÙÙˆÙØ© Ù…Ø±Ø¨Ø¹Ø©.
        Exception: Ø£Ø®Ø·Ø§Ø¡ Ø¹Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø£Ùˆ Ø§Ù„Ø­ÙØ¸.
    """
    if cm.shape[0] != cm.shape[1]:
        logger.error("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø±Ø¨Ø¹Ø©.")
        raise ValueError("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø±Ø¨Ø¹Ø©.")

    if labels is None:
        labels = [str(i) for i in range(cm.shape[0])]

    try:
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.title(title)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path)
            plt.close()
            logger.info(f"Confusion matrix saved to {save_path}")
        else:
            plt.show()
    except Exception as e:
        logger.error(f"Failed to plot confusion matrix: {e}")
        raise


def plot_interactive_confusion_matrix(cm: np.ndarray, labels: list | None = None,
                                      title: str = "Interactive Confusion Matrix",
                                      save_path: str | None = None) -> None:
    """
    Ø±Ø³Ù… ØªÙØ§Ø¹Ù„ÙŠ Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Plotly.

    Args:
        cm (np.ndarray): Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³.
        labels (list, optional): ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª.
        title (str, optional): Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø³Ù….
        save_path (str, optional): Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ù…Ù„Ù HTML.

    Raises:
        ValueError: Ø¥Ø°Ø§ ÙƒØ§Ù† cm Ù„ÙŠØ³Øª Ù…ØµÙÙˆÙØ© Ù…Ø±Ø¨Ø¹Ø©.
        ValueError: Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù…ØªØ¯Ø§Ø¯ save_path .html.
        Exception: Ø£Ø®Ø·Ø§Ø¡ Ø¹Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø£Ùˆ Ø§Ù„Ø­ÙØ¸.
    """
    if cm.shape[0] != cm.shape[1]:
        logger.error("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø±Ø¨Ø¹Ø©.")
        raise ValueError("Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø±Ø¨Ø¹Ø©.")

    x = labels if labels else [str(i) for i in range(cm.shape[0])]
    y = x

    try:
        fig = ff.create_annotated_heatmap(z=cm, x=x, y=y, colorscale='Blues')
        fig.update_layout(
            title_text=title,
            xaxis_title='Predicted',
            yaxis_title='Actual'
        )

        if save_path:
            if not save_path.endswith('.html'):
                logger.error("save_path ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ø§Ù…ØªØ¯Ø§Ø¯ .html")
                raise ValueError("save_path ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ø§Ù…ØªØ¯Ø§Ø¯ .html")
            fig.write_html(save_path)
            logger.info(f"Interactive confusion matrix saved to {save_path}")
            return

        fig.show()
    except Exception as e:
        logger.error(f"Failed to plot interactive confusion matrix: {e}")
        raise


if __name__ == "__main__":
    y_true = [0, 1, 2, 2, 0]
    y_pred = [0, 0, 2, 2, 0]

    print("ğŸ“Š Regression Metrics:")
    print(regression_metrics(y_true, y_pred))

    print("\nğŸ¯ Classification Metrics:")
    cls_metrics = classification_metrics(y_true, y_pred)
    for k in ["Accuracy", "Precision", "Recall", "F1 Score"]:
        print(f"{k}: {cls_metrics[k]}")

    print("\nğŸ“„ Classification Report:")
    report = cls_metrics["Classification Report"]
    for label, metrics in report.items():
        if isinstance(metrics, dict):
            print(f"{label}: ", {m: round(v, 2) for m, v in metrics.items()})
        else:
            print(f"{label}: {round(metrics, 2)}")

    print("\nğŸ“‰ Confusion Matrix:")
    print(cls_metrics["Confusion Matrix"])

    plot_confusion_matrix(cls_metrics["Confusion Matrix"])
