import pandas as pd
import numpy as np
from tabulate import tabulate
import logging
from typing import Dict, Any, Union
from pathlib import Path

# === Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ===
BASE_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = BASE_DIR / "data_intelligence_system" / "data" / "processed"
OUTPUT_DIR = BASE_DIR / "data_intelligence_system" / "analysis" / "analysis_output"

# === Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ===
from data_intelligence_system.analysis.analysis_utils import ensure_output_dir, plot_distribution
from data_intelligence_system.utils.data_loader import load_data
from data_intelligence_system.utils.timer import Timer

# === Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø± ===
logging.basicConfig(level=logging.INFO, format="%(asctime)s â€” %(levelname)s â€” %(message)s")
logger = logging.getLogger(__name__)


def compute_general_stats(df: pd.DataFrame) -> Dict[str, Union[int, float]]:
    """
    ØªØ­Ø³Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø© Ø¹Ù† Ø§Ù„Ø¯Ø§ØªØ§ ÙØ±ÙŠÙ….
    
    Returns:
        dict: Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙØŒ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©ØŒ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Ø¹Ø¯Ø¯ ÙˆÙ†Ø³Ø¨Ø©)ØŒ ÙˆØ§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©.
    """
    total_cells = df.size
    missing_values = df.isnull().sum().sum()
    duplicated_rows = df.duplicated().sum()
    return {
        "Number of Rows": df.shape[0],
        "Number of Columns": df.shape[1],
        "Missing Values": missing_values,
        "Missing %": round((missing_values / total_cells) * 100, 2) if total_cells > 0 else 0.0,
        "Duplicated Rows": duplicated_rows
    }


def compute_numeric_summary(df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
    """
    ØªØ­Ø³Ø¨ Ù…Ù„Ø®Øµ Ø¥Ø­ØµØ§Ø¦ÙŠ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù…Ø¹ Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©.
    
    Returns:
        dict: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù…ÙŠ.
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if numeric_cols.empty:
        return {}
    
    numeric_df = df[numeric_cols]
    desc = numeric_df.describe().T
    missing_counts = numeric_df.isnull().sum()
    missing_percents = (numeric_df.isnull().mean() * 100).round(2)
    
    desc["missing_values"] = missing_counts
    desc["missing_%"] = missing_percents
    return desc.to_dict(orient="index")


def analyze_categorical_columns(df: pd.DataFrame, top_n: int = 10) -> Dict[str, pd.Series]:
    """
    ØªØ­Ù„Ù„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØµÙ†ÙŠÙÙŠØ© ÙˆØªØ¹ÙŠØ¯ ØªÙƒØ±Ø§Ø± Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ….
    """
    categorical_cols = df.select_dtypes(include=["object", "category"]).columns
    cat_summary = {}
    for col in categorical_cols:
        counts = df[col].value_counts(dropna=False).head(top_n)
        cat_summary[col] = counts
        logger.info(f"ğŸ”¤ ØªÙƒØ±Ø§Ø± Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ø¹Ù…ÙˆØ¯ '{col}':\n{counts.to_string()}")
    return cat_summary


def analyze_datetime_columns(df: pd.DataFrame) -> Dict[str, pd.Series]:
    """
    ØªØ­Ù„Ù„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙˆØªØ¹ÙŠØ¯ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø´Ù‡Ø±ÙŠ.
    """
    datetime_cols = df.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns
    datetime_summary = {}
    for col in datetime_cols:
        freq = df[col].dt.to_period("M").value_counts().sort_index()
        datetime_summary[col] = freq
        logger.info(f"â±ï¸ ØªÙˆØ²ÙŠØ¹ Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¹Ù…ÙˆØ¯ '{col}':\n{freq.to_string()}")
    return datetime_summary


def generate_numeric_histograms(df: pd.DataFrame, filename_prefix: str,
                                output_dir: Path = OUTPUT_DIR, bins: int = 30):
    """
    ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©.
    """
    ensure_output_dir(output_dir)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        try:
            plot_distribution(df, column=col,
                              output_name=f"{filename_prefix}_distribution_{col}.png",
                              output_dir=output_dir, bins=bins)
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ Ø±Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ '{col}': {e}")
    logger.info(f"âœ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙŠ: {output_dir}")


@Timer("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„")
def generate_descriptive_stats(df_or_path: Union[pd.DataFrame, str, Path],
                               filename_prefix: str = "output",
                               output_dir: Path = OUTPUT_DIR,
                               save_outputs: bool = True) -> Dict[str, Any]:
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„.
    
    Args:
        df_or_path: Ø¥Ù…Ø§ DataFrame Ø£Ùˆ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª.
        filename_prefix: Ø¨Ø§Ø¯Ø¦Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©.
        output_dir: Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
        save_outputs: Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ True.
    
    Returns:
        dict: Ù…Ù„Ø®ØµØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…ØŒ Ø§Ù„Ø±Ù‚Ù…ÙŠØŒ Ø§Ù„ØªØµÙ†ÙŠÙÙŠØŒ ÙˆØ§Ù„Ø²Ù…Ù†ÙŠ.
    """
    if isinstance(df_or_path, (str, Path)):
        df = load_data(str(df_or_path))
        filename_prefix = Path(df_or_path).stem
        logger.info(f"âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: {df_or_path}")
    elif isinstance(df_or_path, pd.DataFrame):
        df = df_or_path.copy()
    else:
        raise TypeError("Ø§Ù„Ù…Ø¯Ø®Ù„ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† DataFrame Ø£Ùˆ Ù…Ø³Ø§Ø± Ù…Ù„Ù.")

    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„...")

    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¥Ù„Ù‰ ØªÙˆØ§Ø±ÙŠØ® Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
    object_cols = df.select_dtypes(include=["object"]).columns
    for col in object_cols:
        try:
            converted = pd.to_datetime(df[col], errors='coerce')
            if not converted.isnull().all():
                df[col] = converted
        except Exception:
            pass

    general_info = compute_general_stats(df)
    numeric_summary = compute_numeric_summary(df)
    categorical_summary = analyze_categorical_columns(df)
    datetime_summary = analyze_datetime_columns(df)

    if save_outputs:
        ensure_output_dir(output_dir)

        general_info_path = output_dir / f"{filename_prefix}_general_info.txt"
        general_info_path.write_text(
            tabulate(general_info.items(), tablefmt="grid", stralign="left"), encoding="utf-8"
        )
        logger.info(f"ğŸ“„ Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© ÙÙŠ: {general_info_path}")

        numeric_stats_path = output_dir / f"{filename_prefix}_numeric_stats.csv"
        pd.DataFrame(numeric_summary).T.to_csv(numeric_stats_path)
        logger.info(f"ğŸ“„ Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙŠ: {numeric_stats_path}")

        for col, counts in categorical_summary.items():
            cat_path = output_dir / f"{filename_prefix}_value_counts_{col}.csv"
            counts.to_csv(cat_path)
            logger.info(f"ğŸ“„ Ø­ÙØ¸ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ù„Ù„Ø¹Ù…ÙˆØ¯: {col} ÙÙŠ {cat_path}")

        for col, freq in datetime_summary.items():
            datetime_path = output_dir / f"{filename_prefix}_datetime_freq_{col}.csv"
            freq.to_csv(datetime_path)
            logger.info(f"ğŸ“„ Ø­ÙØ¸ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¹Ù…ÙˆØ¯: {col} ÙÙŠ {datetime_path}")

        generate_numeric_histograms(df, filename_prefix, output_dir)

    logger.info("âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­.")
    return {
        "general_info": general_info,
        "numeric_summary": numeric_summary,
        "categorical_summary": categorical_summary,
        "datetime_summary": datetime_summary
    }


if __name__ == "__main__":
    if not DATA_DIR.exists():
        logger.error(f"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {DATA_DIR}")
        exit(1)

    for file in DATA_DIR.iterdir():
        if file.suffix.lower() in [".csv", ".xlsx", ".xls", ".json"] and file.stat().st_size > 0:
            generate_descriptive_stats(file)
