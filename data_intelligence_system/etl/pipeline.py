import logging
from pathlib import Path
from typing import Optional, Union, List, Tuple
from datetime import datetime
import pandas as pd

# âœ… Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ù…Ø­Ø¯Ø«Ø© Ù…Ù† Ø§Ù„Ø¬Ø°Ø±
from data_intelligence_system.etl.transform import transform_datasets
from data_intelligence_system.analysis.descriptive_stats import (
    analyze_numerical_columns,
    analyze_categorical_columns,
    analyze_datetime_columns
)
from data_intelligence_system.etl.extract import extract_file, extract_all_data
from data_intelligence_system.core.data_bindings import save_uploaded_data
from data_intelligence_system.config.paths_config import RAW_DATA_DIR, PROCESSED_DATA_DIR

# ğŸ› ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
LOG_FORMAT = "%(asctime)s â€” %(levelname)s â€” %(name)s â€” %(message)s"
logging.basicConfig(
    level=logging.INFO,
    format=LOG_FORMAT,
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("etl.pipeline")


def analyze_columns(df: pd.DataFrame, name: str):
    logger.info(f"ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù„Ù„Ù…Ù„Ù: {name}")

    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
    datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()

    logger.info(f"   Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©: {numeric_cols}")
    logger.info(f"   Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ©: {categorical_cols}")
    logger.info(f"   Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©: {datetime_cols}")

    if numeric_cols:
        analyze_numerical_columns(df[numeric_cols])
        logger.info("   âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù…ÙƒØªÙ…Ù„.")

    if categorical_cols:
        analyze_categorical_columns(df[categorical_cols])
        logger.info("   âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ù…ÙƒØªÙ…Ù„.")

    if datetime_cols:
        analyze_datetime_columns(df[datetime_cols])
        logger.info("   âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù…ÙƒØªÙ…Ù„.")


def run_full_pipeline(
    filepath: Optional[Union[str, Path]] = None,
    output_dir: Union[str, Path] = PROCESSED_DATA_DIR,
    encode_type: str = 'label',
    scale_type: str = 'standard',
) -> bool:
    """
    ğŸš€ ØªÙ†ÙÙŠØ° Ø´Ø§Ù…Ù„ Ù„Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ ETL:
    - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù Ù…Ø­Ø¯Ø¯ Ø£Ùˆ Ù…Ù† Ù…Ø¬Ù„Ø¯ raw/
    - ØªØ­ÙˆÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ ÙˆØªØ±Ù…ÙŠØ² ÙˆÙ…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©ØŒ Ø§Ù„Ù†ØµÙŠØ© ÙˆØ§Ù„Ø²Ù…Ù†ÙŠØ©
    - Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ processed/
    """
    output_dir = Path(output_dir)
    start_time = datetime.now()
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªÙ†ÙÙŠØ° Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ ETL ...")

    try:
        if filepath:
            filepath = Path(filepath)
            logger.info(f"ğŸ“¥ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„Ù ÙˆØ§Ø­Ø¯: {filepath.name}")
            df_dict = extract_file(filepath)
            datasets = list(df_dict.items())  # extract_file returns Dict[str, DataFrame]
        else:
            logger.info(f"ğŸ“¥ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©")
            datasets = extract_all_data()

        if not datasets:
            logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
            return False

        logger.info("ğŸ§¹ Ø¨Ø¯Ø¡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªÙ†Ø¸ÙŠÙ + ØªØ±Ù…ÙŠØ² + Ù…ÙˆØ§Ø²Ù†Ø©)")
        cleaned_datasets = transform_datasets(
            datasets,
            encode_type=encode_type,
            scale_type=scale_type
        )

        for name, df_clean in cleaned_datasets:
            if df_clean.empty:
                logger.warning(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {name} ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„. ØªÙ… ØªØ®Ø·ÙŠÙ‡.")
                continue

            analyze_columns(df_clean, name)

            clean_name = name.rsplit(".", 1)[0]  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ Ø¥Ù† ÙˆØ¬Ø¯
            saved_path = save_uploaded_data(df_clean, filename=f"cleaned_{clean_name}.csv")
            logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ: {saved_path}")

        elapsed = datetime.now() - start_time
        logger.info(f"âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ø§ÙƒØªÙ…Ù„ Ø®Ù„Ø§Ù„ {elapsed}")
        return True

    except Exception as e:
        logger.exception(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: {e}")
        return False


if __name__ == "__main__":
    success = run_full_pipeline(
        output_dir=PROCESSED_DATA_DIR,
        encode_type='onehot',
        scale_type='minmax',
    )
    if success:
        logger.info("âœ… ETL pipeline Ø§Ù†ØªÙ‡Øª Ø¨Ù†Ø¬Ø§Ø­ ğŸš€")
    else:
        logger.error("âŒ ÙØ´Ù„ ØªÙ†ÙÙŠØ° ETL pipelineØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„.")
