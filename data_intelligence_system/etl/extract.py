from pathlib import Path
import pandas as pd
import logging
from typing import List, Tuple, Union, Dict, Optional

# âœ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø·Ù„Ù‚ Ù…Ù† Ø¬Ø°Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
from data_intelligence_system.etl.etl_utils import log_step, get_all_files, detect_file_type
from data_intelligence_system.utils.file_manager import extract_file_name, read_file
from data_intelligence_system.config.paths_config import RAW_DATA_PATHS, SUPPORTED_EXTENSIONS

try:
    from data_intelligence_system.data.raw.validate_structure import validate_file_structure  # type: ignore
except ImportError:
    validate_file_structure = None

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def is_valid_file(file_path: Path) -> bool:
    return file_path.suffix.lower() in SUPPORTED_EXTENSIONS and file_path.exists()


def try_validate(filepath: Path, validator: Optional[callable]):
    if validator and callable(validator):
        try:
            validator(str(filepath))
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù„Ù {filepath.name}: {e}")


@log_step
def extract_all_data(validate: bool = True) -> List[Tuple[str, pd.DataFrame]]:
    datasets = []

    for data_path in RAW_DATA_PATHS:
        if not data_path.exists():
            logger.warning(f"âš ï¸ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {data_path}")
            continue

        all_files = get_all_files(data_path, extensions=SUPPORTED_EXTENSIONS)
        if not all_files:
            logger.warning(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯: {data_path}")
            continue

        for file_path in map(Path, all_files):
            if not is_valid_file(file_path):
                logger.info(f"â© ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path.name}")
                continue

            try:
                if validate:
                    try_validate(file_path, validate_file_structure)

                df = read_file(str(file_path))
                if not isinstance(df, pd.DataFrame):
                    logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ DataFrame ØµØ§Ù„Ø­ Ù…Ù†: {file_path.name}")
                    continue

                missing = int(df.isnull().sum().sum())
                logger.info(f"âœ… {file_path.name} â†’ Ø´ÙƒÙ„: {df.shape}, Ø£Ø¹Ù…Ø¯Ø©: {len(df.columns)}, Ù…ÙÙ‚ÙˆØ¯Ø§Øª: {missing}")
                datasets.append((file_path.name, df))

            except Exception as e:
                logger.exception(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {file_path.name}: {e}")

    return datasets


@log_step
def extract_file(source_path: Union[str, Path], validate: bool = True) -> Dict[str, pd.DataFrame]:
    file_path = Path(source_path)
    if not is_valid_file(file_path):
        raise FileNotFoundError(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file_path}")

    try:
        if validate:
            try_validate(file_path, validate_file_structure)

        df = read_file(str(file_path))
        if not isinstance(df, pd.DataFrame):
            raise ValueError(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {file_path.name} Ù„Ù… ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ DataFrame Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­")

        file_key = extract_file_name(str(file_path))
        logger.info(f"ğŸ“„ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„Ù: {file_path.name} â†’ {df.shape}")
        return {file_key: df}

    except Exception as e:
        logger.exception(f"âŒ ÙØ´Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„Ù {file_path.name}: {e}")
        raise
