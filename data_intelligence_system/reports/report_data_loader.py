import os
import pandas as pd
from typing import Dict, Any, Optional
from datetime import datetime

from data_intelligence_system.reports.report_config import REPORT_CONFIG
from data_intelligence_system.analysis.descriptive_stats import generate_descriptive_stats
from data_intelligence_system.analysis.correlation_analysis import generate_correlation_matrix
from data_intelligence_system.data.processed.validate_clean_data import validate  # âœ… Ù…Ø¶Ø§Ù Ø­Ø¯ÙŠØ«Ù‹Ø§

class ReportDataLoader:
    """
    Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±.
    ÙŠØ´Ù…Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª CSVØŒ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©ØŒ ÙˆØ§Ù„Ø§Ø±ØªØ¨Ø§Ø·ÙŠØ©.
    ÙŠØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù‘Ù„Ø© Ù„ØªÙØ§Ø¯ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©.
    """

    def __init__(self, processed_data_path: Optional[str] = None):
        base_dir = os.path.abspath(os.path.dirname(__file__))
        default_data_path = os.path.join(base_dir, "..", "data", "processed")
        self.data_path = processed_data_path or default_data_path
        self.loaded_datasets: Dict[str, pd.DataFrame] = {}
        self.metadata: Dict[str, Any] = {}

    def load_all_csvs(self) -> Dict[str, pd.DataFrame]:
        """
        ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª CSV Ù…Ù† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.
        ÙŠÙ‚ÙˆÙ… Ø¨ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ self.loaded_datasets Ù„ØªØ¬Ù†Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©.
        """
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ø³Ø§Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {self.data_path}")

        for file in os.listdir(self.data_path):
            if file.endswith(".csv"):
                filepath = os.path.join(self.data_path, file)
                try:
                    df = pd.read_csv(filepath)
                    if df.empty:
                        print(f"[ØªØ­Ø°ÙŠØ±] Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº: {file} -- ØªÙ… ØªØ®Ø·ÙŠÙ‡")
                        continue

                    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
                    try:
                        validate(df)
                    except Exception as ve:
                        print(f"[ØªØ­Ø°ÙŠØ±] ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„Ù {file}: {ve}")

                    self.loaded_datasets[file] = df

                except pd.errors.EmptyDataError:
                    print(f"[ØªØ­Ø°ÙŠØ±] Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù (ÙØ§Ø±Øº): {file} -- ØªÙ… ØªØ®Ø·ÙŠÙ‡")
                except Exception as e:
                    print(f"[Ø®Ø·Ø£] ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file}: {e} -- ØªÙ… ØªØ®Ø·ÙŠÙ‡")
        return self.loaded_datasets

    def get_dataset(self, filename: str) -> pd.DataFrame:
        """
        Ø¥Ø±Ø¬Ø§Ø¹ DataFrame Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù.
        ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ù…Ù‘Ù„Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¥Ù† ÙˆØ¬Ø¯Øª.
        """
        if filename in self.loaded_datasets:
            return self.loaded_datasets[filename]

        path = os.path.join(self.data_path, filename)
        if not os.path.exists(path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {path}")

        df = pd.read_csv(path)

        # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ±Ø¯ÙŠ
        try:
            validate(df)
        except Exception as ve:
            print(f"[ØªØ­Ø°ÙŠØ±] ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„Ù {filename}: {ve}")

        self.loaded_datasets[filename] = df
        return df

    def generate_statistics(self, filename: str) -> Dict[str, Any]:
        """
        Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØµÙÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ù„Ù Ù…Ø¹ÙŠÙ†.
        """
        full_path = os.path.join(self.data_path, filename)
        generate_descriptive_stats(full_path, output_dir=REPORT_CONFIG["output_dir"])
        return {"status": "generated", "file": filename}

    def generate_correlation(self, filename: str, method: str = "pearson") -> pd.DataFrame:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù…Ù† Ù…Ù„Ù Ù…Ø¹ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© generate_correlation_matrix.
        """
        df = self.get_dataset(filename)
        corr_matrix = generate_correlation_matrix(df, method=method)
        return corr_matrix

    def load_summary_for_report(self, filename: str) -> Dict[str, Any]:
        """
        ØªØ­Ù…ÙŠÙ„ ÙˆØµÙ Ù…ÙˆØ¬Ø² (Ù…Ù„Ø®Øµ) Ù„Ù…Ù„Ù Ù…Ø¹ÙŠÙ† Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ±.
        """
        df = self.get_dataset(filename)
        summary = {
            "file_name": filename,
            "n_rows": df.shape[0],
            "n_cols": df.shape[1],
            "missing_values": int(df.isnull().sum().sum()),
            "missing_%": round(df.isnull().mean().mean() * 100, 2),
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
        return summary

    def get_kpi_summary(self, filename: str) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù„Ø®ØµØ§Øª Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ (KPIs) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ ØªÙ‚Ø§Ø±ÙŠØ± KPI.
        """
        df = self.get_dataset(filename)
        kpis = {
            "total_records": df.shape[0],
            "total_features": df.shape[1],
            "null_rate": round(df.isnull().mean().mean() * 100, 2),
            "last_updated": datetime.now().strftime("%Y-%m-%d"),
        }
        return kpis


if __name__ == "__main__":
    loader = ReportDataLoader()
    datasets = loader.load_all_csvs()
    for name, df in datasets.items():
        print(f"ğŸ”¹ Dataset: {name} | Shape: {df.shape}")
