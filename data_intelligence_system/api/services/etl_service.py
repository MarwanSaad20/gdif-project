from typing import Optional, Dict, Any, List, Tuple
from pathlib import Path
import pandas as pd

from data_intelligence_system.etl.extract import extract_file
from data_intelligence_system.etl.transform import transform_datasets
from data_intelligence_system.etl.load import save_multiple_datasets
from data_intelligence_system.utils.data_loader import load_data
from data_intelligence_system.data.raw.register_sources import main as register_sources_main

from data_intelligence_system.utils.logger import get_logger

logger = get_logger("etl.service")


class ETLService:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.data: Optional[List[Tuple[str, pd.DataFrame]]] = None

    def run_etl(
        self,
        source: str,
        extract_params: Optional[Dict[str, Any]] = None,
        transform_params: Optional[Dict[str, Any]] = None,
        load_params: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        ØªÙ†ÙÙŠØ° ØªØ³Ù„Ø³Ù„ ETL Ø§Ù„ÙƒØ§Ù…Ù„ (Ø§Ø³ØªØ®Ø±Ø§Ø¬ â†’ ØªØ­ÙˆÙŠÙ„ â†’ ØªØ­Ù…ÙŠÙ„)
        """
        try:
            logger.info(f"ðŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ETL Ù„Ù…ØµØ¯Ø±: {source}")

            # ===== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =====
            df = None
            ep = extract_params or {}

            if ep.get("use_load_data", False):
                df = load_data(source)
            else:
                # Ø¯Ø¹Ù… Ù…Ù„ÙØ§Øª external
                source_path = Path(source)
                if ep.get("extract_from_external", False):
                    source_path = (
                        Path(__file__).resolve().parents[2]
                        / "data" / "external" / "downloaded" / source
                    )
                df = extract_file(source_path)

            logger.info(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df.shape} - Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {df.columns.tolist()}")

            # ===== ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =====
            datasets = [(Path(source).stem, df)]
            logger.info(f"ðŸ§¹ Ø¨Ø¯Ø¡ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {transform_params}")
            transformed_datasets = transform_datasets(datasets, **(transform_params or {}))
            if not transformed_datasets or not isinstance(transformed_datasets, list):
                logger.error("âŒ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ´Ù„ Ø£Ùˆ Ù„Ù… ÙŠØ±Ø¬Ø¹ Ù‚Ø§Ø¦Ù…Ø© datasets")
                return False

            logger.info(f"âœ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª = {len(transformed_datasets)}")
            for name, d in transformed_datasets:
                logger.info(f"ðŸ“Š {name}: {d.shape} - Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {d.columns.tolist()}")

            # ===== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª =====
            if not load_params or 'output_dir' not in load_params:
                logger.error("âŒ load_params Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 'output_dir'")
                return False

            logger.info(f"ðŸ’¾ Ø¨Ø¯Ø¡ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {load_params}")
            success = save_multiple_datasets(transformed_datasets, **load_params)
            if not success:
                logger.error("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­ÙˆÙŠÙ„")
                return False

            # âœ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø¹Ø¯ Ø­ÙØ¸Ù‡Ø§
            try:
                register_sources_main()
                logger.info("ðŸ“˜ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„.")
            except Exception as reg_err:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…ØµØ§Ø¯Ø±: {reg_err}")

            self.data = transformed_datasets
            return True

        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙ†ÙÙŠØ° ETL: {e}", exc_info=True)
            return False

    def get_data(self) -> Optional[List[Tuple[str, pd.DataFrame]]]:
        return self.data
